{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4b6bb7-82d5-4bf8-a675-0b87f8ec23a1",
   "metadata": {},
   "source": [
    "# TensorFlow Natural Language Processing\n",
    "\n",
    "NLP has the goal of deriving information out of a language data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af725c3-ff8e-4b82-b532-00200a7c1cc6",
   "metadata": {},
   "source": [
    "## Get helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d0712b-7c07-4b60-b5df-62ed7c3c620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import series helper functions for the notebook\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a490b-003a-43e8-9052-3e9635e1633a",
   "metadata": {},
   "source": [
    "## Get a test dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d083c19-98e2-4ace-915d-767a7938b8b0",
   "metadata": {},
   "source": [
    "## Becoming one with the data\n",
    "\n",
    "Visualizing a text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021109f9-a3d8-4149-b28c-d9e297c8a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdf408c-bf63-411f-b57e-0c3953d507c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa75dcc8-9a9b-4f9a-a5c1-c5cdaeb4c5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>389</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THANKS!!!!! @COUNT DANTE.  :)  DO JOIN US BY F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>8953</td>\n",
       "      <td>storm</td>\n",
       "      <td>mind ya business</td>\n",
       "      <td>@Jenniferarri_ comeeeee! ...but why is it bout...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>7355</td>\n",
       "      <td>obliterate</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>@klavierstuk doesn't so LVG is forced into the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>5313</td>\n",
       "      <td>fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morganite Gemstone White Fire Opal 925 Sterlin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1538</td>\n",
       "      <td>bomb</td>\n",
       "      <td>keli x</td>\n",
       "      <td>HALSEY AND TROYE COLLAB WOULD BE BOMB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>2527</td>\n",
       "      <td>collision</td>\n",
       "      <td>SEATTLE, WA USA</td>\n",
       "      <td>On I-405 southbound at Coal Creek Pkwy there i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>9794</td>\n",
       "      <td>trapped</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>4810</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>Gold Coast, Australia</td>\n",
       "      <td>Tram travellers evacuated after powerlines com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>5187</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>Youngstown, OH</td>\n",
       "      <td>OSP concerned about mounting fatalities http:/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>6935</td>\n",
       "      <td>mass%20murderer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another White mass murderer. Thank God I'm fro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword               location  \\\n",
       "268    389     annihilation                    NaN   \n",
       "6266  8953            storm       mind ya business   \n",
       "5156  7355       obliterate         United Kingdom   \n",
       "3738  5313             fire                    NaN   \n",
       "1066  1538             bomb                 keli x   \n",
       "...    ...              ...                    ...   \n",
       "1756  2527        collision        SEATTLE, WA USA   \n",
       "6837  9794          trapped                    NaN   \n",
       "3360  4810        evacuated  Gold Coast, Australia   \n",
       "3638  5187       fatalities         Youngstown, OH   \n",
       "4871  6935  mass%20murderer                    NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "268   THANKS!!!!! @COUNT DANTE.  :)  DO JOIN US BY F...       0  \n",
       "6266  @Jenniferarri_ comeeeee! ...but why is it bout...       1  \n",
       "5156  @klavierstuk doesn't so LVG is forced into the...       0  \n",
       "3738  Morganite Gemstone White Fire Opal 925 Sterlin...       0  \n",
       "1066              HALSEY AND TROYE COLLAB WOULD BE BOMB       0  \n",
       "...                                                 ...     ...  \n",
       "1756  On I-405 southbound at Coal Creek Pkwy there i...       1  \n",
       "6837  Hollywood Movie About Trapped Miners Released ...       1  \n",
       "3360  Tram travellers evacuated after powerlines com...       1  \n",
       "3638  OSP concerned about mounting fatalities http:/...       1  \n",
       "4871  Another White mass murderer. Thank God I'm fro...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "shuffled_train_df = train_df.sample(frac=1)\n",
    "shuffled_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a724fd-0088-4127-bf20-b6eeba31b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the test dataframe look like\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23371f1c-6733-46a0-8e65-064d0bfe17f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c0f0ee-e68b-431e-a880-77829028874e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total samples?\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500232dd-a765-4b7e-b179-71f2a5fbc39a",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets\n",
    "\n",
    "we will use `sklearn.model_selection.train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ab938c-13b4-42c0-8ebb-4554d6b7c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53ba493d-bfbe-4371-afd1-a83a8febe91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    shuffled_train_df['text'].to_numpy(),\n",
    "    shuffled_train_df['target'].to_numpy(),\n",
    "    test_size = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f082c8d5-5b9a-4745-99d7-e28b31a92992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Remembering Pittsburgh Eyewitness History of Steel City by Len Barcousky PB Penn http://t.co/dhGAVw8bSW http://t.co/0lMhEAEX9k',\n",
       "       'master0fsloths has a crush: http://t.co/SZX6v0bbjF',\n",
       "       'I feel like a tornado http://t.co/iZJK6kpWiZ', ...,\n",
       "       'Ways so archetype a bleeding well-grounded readiness: FpOJ http://t.co/WXbrArc7p3',\n",
       "       '@suelinflower there is no words to describe the physical painthey ripped you apart while you screamed for dear lifeits like been engulfed',\n",
       "       'Repulsive! Refugees-Victimiser-#Dutton Evangelical-Liar-#Abbott c/o #LNP on a dupe the press overdrive; #CHOPPERGATE!#BRONWYNBISHOP!#AUSPOL'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e96e06-b354-4afb-a2a1-f0a17014105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe14db-85d5-458d-b7f0-96c121fe7b1f",
   "metadata": {},
   "source": [
    "## Converting text to numbers\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert text into numbers namely:\n",
    "* Tokenization\n",
    "* Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381cadb6-4a3f-464c-881b-05c0e74c2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fe9d5d-b0db-4395-9160-9e38d1400980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(max_tokens=None,\n",
    "                                    standardize='lower_and_strip_punctuation',\n",
    "                                    split='whitespace',\n",
    "                                    ngrams=None,\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=None,\n",
    "                                    #pad_to_max_tokens=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beddde44-d953-47c9-b32e-ea838dea7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization variables\n",
    "max_vocab_length = 1000 # Max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences wil be\n",
    "text_vect = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode='int',\n",
    "    output_sequence_length = max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bb25765-a36c-4d4c-b075-d97cbffe7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vect.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51e46888-2f5d-4d93-8a20-fd8581222677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[275,   3, 217,   4,  13, 762,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vect([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a791412-f020-42b8-9f7e-494e6d7d796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tex\n",
      "? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n",
      "\n",
      "Vactorized version:[[320   1  90  98   1 685   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f'Original tex\\n{random_sentence}\\n\\nVactorized version:{text_vect([random_sentence])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e57b2e4-639c-4a99-9018-ab0c3fb3b728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 1000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Get all of the unique words in the vocabulary\n",
    "words_in_vocab = text_vect.get_vocabulary()\n",
    "top_5_most_used_words = words_in_vocab[:5]\n",
    "print(f'Number of words in vocab: {len(words_in_vocab)}')\n",
    "print(f'5 most common words: {top_5_most_used_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c72cdb2-bc1c-41a5-862b-77aede54988b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.00647839, -0.0470009 , -0.04536194, ..., -0.01928971,\n",
       "         -0.02454203,  0.04426506],\n",
       "        [ 0.04635367, -0.01618565, -0.02664793, ..., -0.004468  ,\n",
       "         -0.02035745, -0.03338497],\n",
       "        [ 0.00647839, -0.0470009 , -0.04536194, ..., -0.01928971,\n",
       "         -0.02454203,  0.04426506],\n",
       "        ...,\n",
       "        [-0.01785592, -0.01576645,  0.02917823, ...,  0.00204301,\n",
       "          0.00384064,  0.04561888],\n",
       "        [-0.01785592, -0.01576645,  0.02917823, ...,  0.00204301,\n",
       "          0.00384064,  0.04561888],\n",
       "        [-0.01785592, -0.01576645,  0.02917823, ...,  0.00204301,\n",
       "          0.00384064,  0.04561888]]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating and embedding layer\n",
    "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, output_dim=128, input_length=max_length)\n",
    "\n",
    "embedding(text_vect([random.choice(train_sentences)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd69c01-1c1c-4241-a383-3f084d7c38d9",
   "metadata": {},
   "source": [
    "## Modelling a text dataset\n",
    "Now we've got way to turn our text sequences into numbers, it's time to start building a series of modelling experiments.\n",
    "\n",
    "We'll start with a baseline and move on from there.\n",
    "\n",
    "* Model 0: Naive Bayes (baseline), this is a form of Sklearn ML\n",
    "* Model 1: Feed-forward neural network (dense model)\n",
    "* Model 2: LSTM model (RNN)\n",
    "* Model 3: GRU model (RNN)\n",
    "* Model 4: Bidirectional_LSTM model (RNN)\n",
    "* Model 5: 1D Convolutional Neural Network (CNN)\n",
    "* Model 6: TensorFlow Hub Pretrained Feature Extractor (using transfer learning for NLP)\n",
    "* Model 7: Same as model 6 with 10% of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f54e4d-af77-4572-9ecf-d34e43b4475d",
   "metadata": {},
   "source": [
    "### Model 1: Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7bcbdfd2-fc2a-4d7f-b031-5372978e2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model1 = tf.keras.Sequential([\n",
    "    text_vect,\n",
    "    tf.keras.layers.Embedding(\n",
    "        max_vocab_length,\n",
    "        output_dim= 128,\n",
    "    ),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d16e2dab-1f49-4ebb-9adb-96fcc458d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model1.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78535c50-00fc-4425-b1b8-67611ff8667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 128129 (500.50 KB)\n",
      "Trainable params: 128129 (500.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf45fa0c-9f12-4784-a372-50cc24318634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6723 - val_loss: 0.5637 - val_accuracy: 0.7297\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.4910 - accuracy: 0.7799 - val_loss: 0.5103 - val_accuracy: 0.7546\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.8067 - val_loss: 0.4904 - val_accuracy: 0.7625\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.4107 - accuracy: 0.8187 - val_loss: 0.4824 - val_accuracy: 0.7717\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3945 - accuracy: 0.8216 - val_loss: 0.4812 - val_accuracy: 0.7743\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3848 - accuracy: 0.8260 - val_loss: 0.4816 - val_accuracy: 0.7677\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3781 - accuracy: 0.8278 - val_loss: 0.4842 - val_accuracy: 0.7703\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3727 - accuracy: 0.8295 - val_loss: 0.4896 - val_accuracy: 0.7651\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3693 - accuracy: 0.8324 - val_loss: 0.4928 - val_accuracy: 0.7808\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3657 - accuracy: 0.8358 - val_loss: 0.4950 - val_accuracy: 0.7795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22319ed72d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model1.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs = 10,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c70e1a75-04de-4652-bcd5-a6be773ee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6851, 15), dtype=int64, numpy=\n",
       "array([[  1,   1, 580, ...,   1,   1,   0],\n",
       "       [  1,  42,   3, ...,   0,   0,   0],\n",
       "       [  8, 227,  25, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  1,  28,   1, ...,   0,   0,   0],\n",
       "       [  1,  73,   9, ...,   1, 206,  12],\n",
       "       [  1,   1,   1, ...,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = model1.layers[0](train_sentences)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a3ae01a-7962-47ce-8424-9d5e42394437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6851, 15, 1), dtype=float32, numpy=\n",
       "array([[[0.4879135 ],\n",
       "        [0.4879135 ],\n",
       "        [0.5212862 ],\n",
       "        ...,\n",
       "        [0.4879135 ],\n",
       "        [0.4879135 ],\n",
       "        [0.5169261 ]],\n",
       "\n",
       "       [[0.4879135 ],\n",
       "        [0.48872462],\n",
       "        [0.4961161 ],\n",
       "        ...,\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ]],\n",
       "\n",
       "       [[0.49710265],\n",
       "        [0.501212  ],\n",
       "        [0.47803405],\n",
       "        ...,\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.4879135 ],\n",
       "        [0.5014943 ],\n",
       "        [0.4879135 ],\n",
       "        ...,\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ]],\n",
       "\n",
       "       [[0.4879135 ],\n",
       "        [0.49291182],\n",
       "        [0.49931878],\n",
       "        ...,\n",
       "        [0.4879135 ],\n",
       "        [0.50083894],\n",
       "        [0.5027638 ]],\n",
       "\n",
       "       [[0.4879135 ],\n",
       "        [0.4879135 ],\n",
       "        [0.4879135 ],\n",
       "        ...,\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ],\n",
       "        [0.5169261 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = model1.layers[2](result2)\n",
    "result3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab8a83-b05b-4afb-9779-cd75d1532aa1",
   "metadata": {},
   "source": [
    "## Model 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fcdb6a71-3278-4f09-ad8f-a14f3cca135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model2 = tf.keras.Sequential([\n",
    "    text_vect,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = max_vocab_length,\n",
    "        output_dim = 128\n",
    "    ),\n",
    "    tf.keras.layers.LSTM(units=64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=64),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c48bd12-c3a1-4bb8-b6f6-9356012bb7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_10 (Embedding)    (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 15, 64)            49408     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 210497 (822.25 KB)\n",
      "Trainable params: 210497 (822.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "399980bb-3a58-4523-936f-f77a54a7ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9239868f-0353-4505-948f-609ccf768216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - 8s 22ms/step - loss: 0.5251 - accuracy: 0.7399 - val_loss: 0.5146 - val_accuracy: 0.7362\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.4211 - accuracy: 0.8114 - val_loss: 0.4699 - val_accuracy: 0.7743\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.3985 - accuracy: 0.8246 - val_loss: 0.4688 - val_accuracy: 0.7848\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.3774 - accuracy: 0.8400 - val_loss: 0.4702 - val_accuracy: 0.7874\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.3498 - accuracy: 0.8499 - val_loss: 0.4880 - val_accuracy: 0.7651\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.3275 - accuracy: 0.8586 - val_loss: 0.5568 - val_accuracy: 0.7743\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.2966 - accuracy: 0.8765 - val_loss: 0.6223 - val_accuracy: 0.7703\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.2779 - accuracy: 0.8841 - val_loss: 0.6254 - val_accuracy: 0.7769\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.2572 - accuracy: 0.8923 - val_loss: 0.7699 - val_accuracy: 0.7559\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.2325 - accuracy: 0.9056 - val_loss: 0.7318 - val_accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history2 = model2.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632213a1-c196-498e-ab92-331bf24e944b",
   "metadata": {},
   "source": [
    "#### Visualize our model with https://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8562ab52-f8ad-49c3-be2a-006ac6fe3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model2.get_layer('embedding_8').get_weights()[0]\n",
    "vocab = text_vect.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23f29b50-2861-4cab-a123-8e03fd69fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a4cdb-7300-41cc-b5fd-324e6d2a2668",
   "metadata": {},
   "source": [
    "## Model 3: Using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3386480b-f388-491e-b0ed-e73602c116e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model3 = tf.keras.Sequential([\n",
    "    text_vect,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = max_vocab_length,\n",
    "        output_dim = 128\n",
    "    ),\n",
    "    tf.keras.layers.GRU(64, return_sequences=True),\n",
    "    tf.keras.layers.GRU(64),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4cb6d818-7646-472c-9d65-f41f71ec5020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_12 (Embedding)    (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 15, 64)            37248     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 190273 (743.25 KB)\n",
      "Trainable params: 190273 (743.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "08300b1e-4281-4226-b7ac-7543811a10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model3.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "58e51f3c-504d-4ddc-9b7c-85ce35db7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - 8s 20ms/step - loss: 0.5362 - accuracy: 0.7215 - val_loss: 0.5074 - val_accuracy: 0.7480\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.4198 - accuracy: 0.8129 - val_loss: 0.5068 - val_accuracy: 0.7638\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3954 - accuracy: 0.8273 - val_loss: 0.4747 - val_accuracy: 0.7822\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3680 - accuracy: 0.8432 - val_loss: 0.5107 - val_accuracy: 0.7769\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3433 - accuracy: 0.8575 - val_loss: 0.5613 - val_accuracy: 0.7861\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3198 - accuracy: 0.8654 - val_loss: 0.5316 - val_accuracy: 0.7612\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2936 - accuracy: 0.8793 - val_loss: 0.5524 - val_accuracy: 0.7717\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2731 - accuracy: 0.8891 - val_loss: 0.6033 - val_accuracy: 0.7402\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2563 - accuracy: 0.8974 - val_loss: 0.6358 - val_accuracy: 0.7402\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2439 - accuracy: 0.9032 - val_loss: 0.6599 - val_accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history3 = model3.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d488682-0b6c-426a-9c0f-89ab8eb21537",
   "metadata": {},
   "source": [
    "## Model 3: Using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d46b802-ec8b-4830-810b-a2e672bd31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model3 = tf.keras.Sequential([\n",
    "    text_vect,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = max_vocab_length,\n",
    "        output_dim = 128\n",
    "    ),\n",
    "    tf.keras.layers.GRU(64, return_sequences=True),\n",
    "    tf.keras.layers.GRU(64),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bb728c88-90c0-49ed-b573-2852a8b989e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_12 (Embedding)    (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 15, 64)            37248     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 190273 (743.25 KB)\n",
      "Trainable params: 190273 (743.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93147279-df67-408b-b6d2-54c52c5ed285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model3.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57941bd2-d23f-45c6-8ecd-2e8aa1c50ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - 8s 20ms/step - loss: 0.5362 - accuracy: 0.7215 - val_loss: 0.5074 - val_accuracy: 0.7480\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.4198 - accuracy: 0.8129 - val_loss: 0.5068 - val_accuracy: 0.7638\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3954 - accuracy: 0.8273 - val_loss: 0.4747 - val_accuracy: 0.7822\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3680 - accuracy: 0.8432 - val_loss: 0.5107 - val_accuracy: 0.7769\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3433 - accuracy: 0.8575 - val_loss: 0.5613 - val_accuracy: 0.7861\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3198 - accuracy: 0.8654 - val_loss: 0.5316 - val_accuracy: 0.7612\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2936 - accuracy: 0.8793 - val_loss: 0.5524 - val_accuracy: 0.7717\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2731 - accuracy: 0.8891 - val_loss: 0.6033 - val_accuracy: 0.7402\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2563 - accuracy: 0.8974 - val_loss: 0.6358 - val_accuracy: 0.7402\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2439 - accuracy: 0.9032 - val_loss: 0.6599 - val_accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history3 = model3.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b390b95-157a-446e-b3f9-c27c39edcb39",
   "metadata": {},
   "source": [
    "## Model 4: Bidirectional LSTM x GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e34c989-f993-49bd-8924-0c27159900bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model4 = tf.keras.Sequential([\n",
    "    text_vect,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = max_vocab_length,\n",
    "        output_dim=128\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(units=64, return_sequences=True)\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(units=64)\n",
    "    ),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6807c6b0-2d3d-4d0b-863f-9460d0d9ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, 15, 128)           98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 128)               74496     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301441 (1.15 MB)\n",
      "Trainable params: 301441 (1.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5c93ae57-0f8a-4357-86b3-24e26795c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model4.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "01ed0548-667d-45eb-885f-19feca0afb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - 12s 29ms/step - loss: 0.5152 - accuracy: 0.7402 - val_loss: 0.4642 - val_accuracy: 0.7782\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.4166 - accuracy: 0.8114 - val_loss: 0.4685 - val_accuracy: 0.7808\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3873 - accuracy: 0.8295 - val_loss: 0.4643 - val_accuracy: 0.7769\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3549 - accuracy: 0.8476 - val_loss: 0.4998 - val_accuracy: 0.7703\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3258 - accuracy: 0.8621 - val_loss: 0.5454 - val_accuracy: 0.7598\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2984 - accuracy: 0.8734 - val_loss: 0.5708 - val_accuracy: 0.7690\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2723 - accuracy: 0.8864 - val_loss: 0.5638 - val_accuracy: 0.7795\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2402 - accuracy: 0.8999 - val_loss: 0.6861 - val_accuracy: 0.7572\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.2153 - accuracy: 0.9113 - val_loss: 0.7137 - val_accuracy: 0.7572\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1912 - accuracy: 0.9206 - val_loss: 0.8060 - val_accuracy: 0.7428\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history4 = model4.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15b581-6fe5-4d9d-a32f-e4af5671520a",
   "metadata": {},
   "source": [
    "## Model 5: CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3fa67a71-e3b2-41d2-912a-99b9b2a2e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model5 = tf.keras.Sequential([\n",
    "    text_vect,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim = max_vocab_length,\n",
    "        output_dim=128\n",
    "    ),\n",
    "    tf.keras.layers.Conv1D(10, 3),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ea6fae0f-6dfd-44ef-9d8d-9578815728ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_18 (Embedding)    (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 13, 10)            3850      \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (None, 10)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131861 (515.08 KB)\n",
      "Trainable params: 131861 (515.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77385e66-462f-4b2c-98d4-f7b1328c57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model5.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0483203a-4474-4652-9c83-f0e96d43a739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3785 - accuracy: 0.8281 - val_loss: 0.5253 - val_accuracy: 0.7507\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8302 - val_loss: 0.5170 - val_accuracy: 0.7638\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3702 - accuracy: 0.8313 - val_loss: 0.5216 - val_accuracy: 0.7520\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3668 - accuracy: 0.8368 - val_loss: 0.5299 - val_accuracy: 0.7520\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3663 - accuracy: 0.8335 - val_loss: 0.5301 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history5 = model5.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fa8f0-7903-4372-b474-128bc1917697",
   "metadata": {},
   "source": [
    "## Transfer Learning Feature Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922d303-540c-44bd-8b6b-523fcb19dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56102860-a04a-4ad2-8c35-1900767d6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model6 = tf.keras.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                  input_shape=[],\n",
    "                  dtype=tf.string,\n",
    "                  trainable=False),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586af6ab-2d84-49f8-82ba-187e339240e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model6.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a03a8-bd3a-4f1c-8e1e-5c1be8a9fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history6 = model6.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029ca79-c2a8-431a-9dfa-6864cbd5b93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
